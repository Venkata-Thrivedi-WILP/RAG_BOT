{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e5fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing UserManagement.json...\n",
      "Created ../processed_data/UserManagement.txt with 33 endpoints\n",
      "Processing PolicyMangement.json...\n",
      "Created ../processed_data/PolicyMangement.txt with 9 endpoints\n",
      "Processing ApplicationManagement.json...\n",
      "Created ../processed_data/ApplicationManagement.txt with 35 endpoints\n",
      "Processing complete. 77 total endpoints from 3 files.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def load_openapi_spec(file_path):\n",
    "    \"\"\"Load and parse the OpenAPI JSON specification.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def clean_html(text):\n",
    "    \"\"\"Remove HTML tags and clean up formatting.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"No description provided\"\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    return clean_text.strip()\n",
    "\n",
    "def extract_extension_fields(method_info):\n",
    "    \"\"\"Extract OpenAPI extension fields starting with 'x-'.\"\"\"\n",
    "    extensions = {}\n",
    "    for key, value in method_info.items():\n",
    "        if key.startswith('x-'):\n",
    "            extensions[key] = value\n",
    "    return extensions\n",
    "\n",
    "def resolve_schema_reference(ref_path, openapi_spec):\n",
    "    \"\"\"Resolve a schema reference to its actual schema definition.\"\"\"\n",
    "    if not ref_path.startswith('#/'):\n",
    "        return None  # External references not supported\n",
    "    \n",
    "    # Remove '#/' from the path and split by '/'\n",
    "    path_parts = ref_path.lstrip('#/').split('/')\n",
    "    \n",
    "    # Navigate through the OpenAPI spec to find the referenced schema\n",
    "    current = openapi_spec\n",
    "    for part in path_parts:\n",
    "        if part in current:\n",
    "            current = current[part]\n",
    "        else:\n",
    "            return None  # Reference path not found\n",
    "    \n",
    "    return current\n",
    "\n",
    "def generate_sample_json(schema, openapi_spec=None, is_root=True):\n",
    "    \"\"\"Generate a sample JSON object from an OpenAPI schema.\"\"\"\n",
    "    if not schema:\n",
    "        return None\n",
    "    \n",
    "    # Handle $ref references\n",
    "    if '$ref' in schema and openapi_spec:\n",
    "        # Resolve the reference to get the actual schema\n",
    "        resolved_schema = resolve_schema_reference(schema['$ref'], openapi_spec)\n",
    "        if resolved_schema:\n",
    "            return generate_sample_json(resolved_schema, openapi_spec, is_root)\n",
    "        return {\"$ref\": schema['$ref']}  # Fallback if resolution fails\n",
    "    \n",
    "    # Handle different schema types\n",
    "    schema_type = schema.get('type')\n",
    "    \n",
    "    if schema_type == 'object':\n",
    "        result = {}\n",
    "        if 'properties' in schema:\n",
    "            for prop_name, prop_schema in schema['properties'].items():\n",
    "                result[prop_name] = generate_sample_json(prop_schema, openapi_spec, False)\n",
    "        return result\n",
    "    \n",
    "    elif schema_type == 'array':\n",
    "        if 'items' in schema:\n",
    "            return [generate_sample_json(schema['items'], openapi_spec, False)]\n",
    "        return []\n",
    "    \n",
    "    elif schema_type == 'string':\n",
    "        return \"string_value\"\n",
    "    \n",
    "    elif schema_type == 'number' or schema_type == 'integer':\n",
    "        return 0\n",
    "    \n",
    "    elif schema_type == 'boolean':\n",
    "        return False\n",
    "    \n",
    "    # If no specific type or unsupported type\n",
    "    return None\n",
    "\n",
    "def format_property(name, details, indent=\"\"):\n",
    "    \"\"\"Format a schema property in a RAG-friendly way.\"\"\"\n",
    "    prop_type = details.get('type', 'undefined')\n",
    "    description = details.get('description', 'No description provided')\n",
    "    description = clean_html(description)\n",
    "    \n",
    "    if description == '{â€¦}':\n",
    "        description = \"Additional nested properties (abbreviated in schema)\"\n",
    "    \n",
    "    return f\"{indent}* {name} ({prop_type}): {description}\"\n",
    "\n",
    "def format_schema_properties(schema, indent_level=0):\n",
    "    \"\"\"Recursively format schema properties.\"\"\"\n",
    "    lines = []\n",
    "    indent = \"  \" * indent_level\n",
    "    \n",
    "    if not schema:\n",
    "        return lines\n",
    "    \n",
    "    if '$ref' in schema:\n",
    "        ref = schema['$ref'].split('/')[-1]\n",
    "        return [f\"{indent}References schema: {ref}\"]\n",
    "    \n",
    "    if 'properties' in schema:\n",
    "        for prop_name, prop_details in schema['properties'].items():\n",
    "            lines.append(format_property(prop_name, prop_details, indent))\n",
    "            \n",
    "            # Handle nested objects\n",
    "            if prop_details.get('type') == 'object' and 'properties' in prop_details:\n",
    "                lines.append(f\"{indent}  Nested properties:\")\n",
    "                for nested_lines in format_schema_properties(prop_details, indent_level + 2):\n",
    "                    lines.append(nested_lines)\n",
    "            \n",
    "            # Handle arrays\n",
    "            if prop_details.get('type') == 'array' and 'items' in prop_details:\n",
    "                lines.append(f\"{indent}  Array items:\")\n",
    "                for nested_lines in format_schema_properties(prop_details['items'], indent_level + 2):\n",
    "                    lines.append(nested_lines)\n",
    "    \n",
    "    if 'required' in schema and schema['required']:\n",
    "        lines.append(f\"{indent}Required fields: {', '.join(schema['required'])}\")\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def format_endpoint(path, method_info, http_method, openapi_spec):\n",
    "    \"\"\"Format a single endpoint into RAG-friendly text.\"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    # Basic endpoint information\n",
    "    title = method_info.get('summary', 'Unnamed Endpoint')\n",
    "    lines.append(f\"ENDPOINT: {title}\")\n",
    "    lines.append(f\"PATH: {path}\")\n",
    "    lines.append(f\"METHOD: {http_method.upper()}\")\n",
    "    \n",
    "    # Tags\n",
    "    if 'tags' in method_info:\n",
    "        lines.append(f\"TAGS: {', '.join(method_info['tags'])}\")\n",
    "    \n",
    "    # Description\n",
    "    if 'description' in method_info:\n",
    "        description = clean_html(method_info['description'])\n",
    "        lines.append(f\"DESCRIPTION: {description}\")\n",
    "    \n",
    "    # Extension fields (metadata)\n",
    "    extensions = extract_extension_fields(method_info)\n",
    "    if extensions:\n",
    "        lines.append(\"METADATA:\")\n",
    "        for ext_key, ext_value in extensions.items():\n",
    "            lines.append(f\"  * {ext_key}: {ext_value}\")\n",
    "    \n",
    "    # Parameters\n",
    "    if 'parameters' in method_info and method_info['parameters']:\n",
    "        lines.append(\"PARAMETERS:\")\n",
    "        for param in method_info['parameters']:\n",
    "            param_name = param.get('name', 'unnamed')\n",
    "            param_in = param.get('in', 'undefined')\n",
    "            param_required = \"Required\" if param.get('required', False) else \"Optional\"\n",
    "            param_description = clean_html(param.get('description', 'No description provided'))\n",
    "            lines.append(f\"  * {param_name} ({param_in}, {param_required}): {param_description}\")\n",
    "    \n",
    "    # Request Body\n",
    "    if 'requestBody' in method_info:\n",
    "        req_body = method_info['requestBody']\n",
    "        req_required = \"Required\" if req_body.get('required', False) else \"Optional\"\n",
    "        lines.append(f\"REQUEST BODY: {req_required}\")\n",
    "        \n",
    "        if 'content' in req_body:\n",
    "            for content_type, content_details in req_body['content'].items():\n",
    "                lines.append(f\"  Content Type: {content_type}\")\n",
    "                \n",
    "                if 'schema' in content_details:\n",
    "                    schema = content_details['schema']\n",
    "                    lines.append(\"  Schema Properties:\")\n",
    "                    lines.extend(format_schema_properties(schema, 2))\n",
    "                    \n",
    "                    # Generate sample request JSON\n",
    "                    sample_json = generate_sample_json(schema, openapi_spec)\n",
    "                    if sample_json:\n",
    "                        lines.append(\"  Sample Request JSON:\")\n",
    "                        lines.append(f\"  ```json\\n  {json.dumps(sample_json, indent=2)}\\n  ```\")\n",
    "    \n",
    "    # Responses\n",
    "    if 'responses' in method_info:\n",
    "        lines.append(\"RESPONSES:\")\n",
    "        for status_code, response_info in method_info['responses'].items():\n",
    "            lines.append(f\"  Status Code: {status_code}\")\n",
    "            description = clean_html(response_info.get('description', 'No description provided'))\n",
    "            lines.append(f\"  Description: {description}\")\n",
    "            \n",
    "            if 'content' in response_info:\n",
    "                for content_type, content_details in response_info['content'].items():\n",
    "                    lines.append(f\"  Content Type: {content_type}\")\n",
    "                    \n",
    "                    if 'schema' in content_details:\n",
    "                        schema = content_details['schema']\n",
    "                        # In the responses section where it handles $ref\n",
    "                        if '$ref' in schema:\n",
    "                            ref_path = schema['$ref']\n",
    "                            schema_name = ref_path.split('/')[-1]\n",
    "                            \n",
    "                            # Instead of just referencing the schema name\n",
    "                            # lines.append(f\"  Response Schema: References {schema_name}\")\n",
    "                            \n",
    "                            # Get the full schema from components\n",
    "                            full_schema = None\n",
    "                            if ref_path.startswith('#/components/schemas/'):\n",
    "                                schema_name = ref_path.split('/')[-1]\n",
    "                                if 'components' in openapi_spec and 'schemas' in openapi_spec['components'] and schema_name in openapi_spec['components']['schemas']:\n",
    "                                    full_schema = openapi_spec['components']['schemas'][schema_name]\n",
    "                            \n",
    "                            if full_schema:\n",
    "                                lines.append(f\"  Response Schema: {schema_name}\")\n",
    "                                # Add schema description if available\n",
    "                                if 'description' in full_schema:\n",
    "                                    description = clean_html(full_schema.get('description', 'No description provided'))\n",
    "                                    lines.append(f\"  Description: {description}\")\n",
    "                                \n",
    "                                lines.append(\"  Response Body Properties:\")\n",
    "                                lines.extend(format_schema_properties(full_schema, 2))\n",
    "                                \n",
    "                                # Generate sample response JSON\n",
    "                                sample_json = generate_sample_json(full_schema, openapi_spec)\n",
    "                                if sample_json:\n",
    "                                    lines.append(\"  Sample Response JSON:\")\n",
    "                                    lines.append(f\"  ```json\\n  {json.dumps(sample_json, indent=2)}\\n  ```\")\n",
    "                            else:\n",
    "                                # Fallback to just showing the reference if we can't find the schema\n",
    "                                lines.append(f\"  Response Schema: References {schema_name}\")\n",
    "                                resolved_schema = resolve_schema_reference(schema['$ref'], openapi_spec)\n",
    "                                if resolved_schema:\n",
    "                                    lines.append(\"  Response Schema Properties:\")\n",
    "                                    lines.extend(format_schema_properties(resolved_schema, 2))\n",
    "                                    \n",
    "                                    # Generate sample response JSON from resolved schema\n",
    "                                    sample_json = generate_sample_json(resolved_schema, openapi_spec)\n",
    "                                    if sample_json:\n",
    "                                        lines.append(\"  Sample Response JSON:\")\n",
    "                                        lines.append(f\"  ```json\\n  {json.dumps(sample_json, indent=2)}\\n  ```\")\n",
    "    \n",
    "    # Security\n",
    "    if 'security' in method_info:\n",
    "        security_schemes = []\n",
    "        for security_item in method_info['security']:\n",
    "            for scheme, scopes in security_item.items():\n",
    "                security_schemes.append(scheme)\n",
    "        if security_schemes:\n",
    "            lines.append(f\"SECURITY: {', '.join(security_schemes)}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def format_all_endpoints(openapi_spec):\n",
    "    \"\"\"Process all endpoints in the OpenAPI specification.\"\"\"\n",
    "    all_endpoints = []\n",
    "    \n",
    "    # API info\n",
    "    info = openapi_spec.get('info', {})\n",
    "    api_title = info.get('title', 'Unnamed API')\n",
    "    api_version = info.get('version', 'Unknown Version')\n",
    "    \n",
    "    # all_endpoints.append(f\"API: {api_title}\")\n",
    "    # all_endpoints.append(f\"VERSION: {api_version}\")\n",
    "    # all_endpoints.append(\"\")\n",
    "    \n",
    "    # Process each path and method\n",
    "    for path, path_item in openapi_spec.get('paths', {}).items():\n",
    "        for method, method_info in path_item.items():\n",
    "            # Skip non-HTTP methods\n",
    "            if method in ['parameters', 'servers', 'summary', 'description']:\n",
    "                continue\n",
    "                \n",
    "            endpoint_text = format_endpoint(path, method_info, method, openapi_spec)\n",
    "            all_endpoints.append(endpoint_text)\n",
    "            all_endpoints.append(\"--------\" * 10) \n",
    "    \n",
    "    # Remove the COMPONENT SCHEMAS section completely\n",
    "    \n",
    "    return \"\\n\".join(all_endpoints)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Process all OpenAPI spec files in the json_files directory.\"\"\"\n",
    "    # Define directories\n",
    "    json_dir = \"./json_files/\"\n",
    "    output_dir = \"../processed_data/\"\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get list of JSON files\n",
    "    json_files = [os.path.join(json_dir, f) for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {json_dir}\")\n",
    "        return\n",
    "\n",
    "    # Process each file\n",
    "    endpoint_count = 0\n",
    "\n",
    "    for json_file in json_files:\n",
    "        # Get base filename without extension\n",
    "        base_name = os.path.basename(json_file)\n",
    "        file_name_without_ext = os.path.splitext(base_name)[0]\n",
    "        output_file = os.path.join(output_dir, f\"{file_name_without_ext}.txt\")\n",
    "        \n",
    "        # Process the file\n",
    "        print(f\"Processing {base_name}...\")\n",
    "        api_spec = load_openapi_spec(json_file)\n",
    "        api_text = format_all_endpoints(api_spec)\n",
    "        \n",
    "        # Count endpoints\n",
    "        endpoint_lines = [line for line in api_text.split('\\n') if line.startswith(\"ENDPOINT:\")]\n",
    "        endpoint_count += len(endpoint_lines)\n",
    "        \n",
    "        # Write individual file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(api_text)\n",
    "        \n",
    "        print(f\"Created {output_file} with {len(endpoint_lines)} endpoints\")\n",
    "\n",
    "    print(f\"Processing complete. {endpoint_count} total endpoints from {len(json_files)} files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3035deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from git import Repo\n",
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1d765",
   "metadata": {},
   "source": [
    "## Step 3: Configure Private Git Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd076221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure private Git repository details\n",
    "def setup_private_repo(repo_url, auth_token):\n",
    "    \"\"\"\n",
    "    Set up credentials for private Git repository\n",
    "    \n",
    "    Args:\n",
    "        repo_url (str): URL of the private Git repository\n",
    "        auth_token (str): Authentication token for the private repository\n",
    "    \n",
    "    Returns:\n",
    "        str: The repository URL with embedded authentication token\n",
    "    \"\"\"\n",
    "    # Parse repository URL\n",
    "    parsed_url = urlparse(repo_url)\n",
    "    \n",
    "    # Construct repository URL with authentication\n",
    "    if parsed_url.scheme == \"https\":\n",
    "        # Format: https://{token}@github.com/username/repo.git\n",
    "        auth_url = f\"https://{auth_token}@{parsed_url.netloc}{parsed_url.path}\"\n",
    "    else:\n",
    "        # If not HTTPS, keep URL as is and rely on other authentication methods\n",
    "        logger.warning(\"Non-HTTPS repository URL provided. Token authentication might not work.\")\n",
    "        auth_url = repo_url\n",
    "        \n",
    "    return auth_url\n",
    "\n",
    "# Set your private repository details here\n",
    "private_repo_url = \"https://github.com/yourusername/your-private-repo.git\"  # Replace with your repository URL\n",
    "auth_token = \"your-auth-token\"  # Replace with your personal access token\n",
    "\n",
    "# Create authenticated repository URL\n",
    "authenticated_repo_url = setup_private_repo(private_repo_url, auth_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410fe867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_to_git(repo_path, files_to_commit, commit_message):\n",
    "    \"\"\"Commit files to a Git repository\"\"\"\n",
    "    try:\n",
    "        # Initialize repository\n",
    "        repo = Repo(repo_path)\n",
    "        \n",
    "        # Check if repo is dirty (has uncommitted changes)\n",
    "        if repo.is_dirty(untracked_files=True):\n",
    "            # Add files\n",
    "            for file_path in files_to_commit:\n",
    "                relative_path = os.path.relpath(file_path, repo_path)\n",
    "                repo.git.add(relative_path)\n",
    "            \n",
    "            # Commit changes\n",
    "            repo.git.commit('-m', commit_message)\n",
    "            logger.info(f\"Committed {len(files_to_commit)} files to repository\")\n",
    "            \n",
    "            # You could add push here if needed\n",
    "            # repo.git.push()\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            logger.info(\"No changes to commit\")\n",
    "            return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Git error: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4429ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commit_to_private_repo(repo_url, files_to_commit, commit_message):\n",
    "    \"\"\"\n",
    "    Clone private repository, add files, commit and push changes\n",
    "    \n",
    "    Args:\n",
    "        repo_url (str): URL of the private Git repository with authentication token\n",
    "        files_to_commit (list): List of file paths to commit\n",
    "        commit_message (str): Commit message\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    temp_dir = '/tmp/private_repo_clone'\n",
    "    \n",
    "    try:\n",
    "        # Remove temp directory if it exists\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)\n",
    "        \n",
    "        # Clone the repository\n",
    "        logger.info(f\"Cloning private repository...\")\n",
    "        repo = Repo.clone_from(repo_url, temp_dir)\n",
    "        \n",
    "        # Create target directory in the cloned repo\n",
    "        target_dir = os.path.join(temp_dir, 'text_files')\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy files to the target directory\n",
    "        for file_path in files_to_commit:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            target_path = os.path.join(target_dir, file_name)\n",
    "            shutil.copy2(file_path, target_path)\n",
    "            logger.info(f\"Copied {file_path} to {target_path}\")\n",
    "        \n",
    "        # Add all files\n",
    "        repo.git.add(A=True)\n",
    "        \n",
    "        # Check if there are changes to commit\n",
    "        if repo.is_dirty(untracked_files=True):\n",
    "            # Commit changes\n",
    "            repo.git.commit('-m', commit_message)\n",
    "            logger.info(f\"Committed {len(files_to_commit)} files to private repository\")\n",
    "            \n",
    "            # Push changes\n",
    "            logger.info(\"Pushing changes to private repository...\")\n",
    "            repo.git.push()\n",
    "            logger.info(\"Successfully pushed changes to private repository\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            logger.info(\"No changes to commit in private repository\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error with private repository: {str(e)}\")\n",
    "        return False\n",
    "    finally:\n",
    "        # Clean up - remove temp directory\n",
    "        if os.path.exists(temp_dir):\n",
    "            shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb57c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 07:40:42,464 - INFO - Found 3 text files in ../processed_data\n",
      "2025-06-07 07:40:42,465 - INFO - Cloning private repository...\n",
      "2025-06-07 07:40:43,670 - INFO - Copied ../processed_data/UserManagement.txt to /tmp/private_repo_clone/text_files/UserManagement.txt\n",
      "2025-06-07 07:40:43,671 - INFO - Copied ../processed_data/ApplicationManagement.txt to /tmp/private_repo_clone/text_files/ApplicationManagement.txt\n",
      "2025-06-07 07:40:43,672 - INFO - Copied ../processed_data/PolicyMangement.txt to /tmp/private_repo_clone/text_files/PolicyMangement.txt\n",
      "2025-06-07 07:40:43,692 - INFO - Committed 3 files to private repository\n",
      "2025-06-07 07:40:43,693 - INFO - Pushing changes to private repository...\n",
      "2025-06-07 07:40:44,730 - INFO - Successfully pushed changes to private repository\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully committed files to private repository\n"
     ]
    }
   ],
   "source": [
    "import datetime as date\n",
    "# Commit the files to the repository\n",
    "commit_message = \"Add converted JSON files from YAML sources - }\" + date.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Read text files from the processed_data directory\n",
    "processed_data_dir = \"../processed_data\"\n",
    "text_files = []\n",
    "\n",
    "# Check if directory exists\n",
    "if os.path.exists(processed_data_dir):\n",
    "    # Get all text files\n",
    "    text_files = [os.path.join(processed_data_dir, f) for f in os.listdir(processed_data_dir) \n",
    "                 if f.endswith('.txt')]\n",
    "    logger.info(f\"Found {len(text_files)} text files in {processed_data_dir}\")\n",
    "else:\n",
    "    logger.warning(f\"Directory {processed_data_dir} does not exist\")\n",
    "\n",
    "if text_files:\n",
    "    # # For local workspace repository (original approach)\n",
    "    # workspace_repo_path = '/workspaces/RAG_BOT'\n",
    "    # workspace_success = commit_to_git(workspace_repo_path, json_files, commit_message)\n",
    "    \n",
    "    # if workspace_success:\n",
    "    #     print(\"Successfully committed files to workspace repository\")\n",
    "    # else:\n",
    "    #     print(\"Failed to commit files to workspace repository\")\n",
    "    \n",
    "    # For private repository (new approach)\n",
    "    # Uncomment and fill in the details when ready to use\n",
    "    auth_token = \"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\"\n",
    "    private_repo_url = \"https://github.com/Venkata-Thrivedi-WILP/DataStore.git\"\n",
    "    authenticated_repo_url = setup_private_repo(private_repo_url, auth_token)\n",
    "    private_success = commit_to_private_repo(authenticated_repo_url, text_files, commit_message)\n",
    "    \n",
    "    if private_success:\n",
    "        print(\"Successfully committed files to private repository\")\n",
    "    else:\n",
    "        print(\"Failed to commit files to private repository\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
