{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c05ec01",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24852b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4126bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a314bdb",
   "metadata": {},
   "source": [
    "## Document Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29beacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(folder_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Read all text documents from the given folder and return as LangChain Document objects.\n",
    "    Documents are sorted by filename to maintain order.\n",
    "    \"\"\"\n",
    "    print(f\"Reading documents from {folder_path}...\")\n",
    "    files = sorted(glob.glob(os.path.join(folder_path, \"*.txt\")))\n",
    "    documents = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            doc_name = Path(file_path).stem\n",
    "            doc_index = int(doc_name.split('_')[-1]) if doc_name.split('_')[-1].isdigit() else len(documents)\n",
    "            \n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": file_path, \n",
    "                        \"doc_name\": doc_name, \n",
    "                        \"doc_index\": doc_index\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Sort documents by their index to ensure correct ordering\n",
    "    documents.sort(key=lambda x: x.metadata[\"doc_index\"])\n",
    "    \n",
    "    print(f\"Found {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da7f92",
   "metadata": {},
   "source": [
    "## Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55efc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "    Maintains document boundaries and stores adjacent document references.\n",
    "    \"\"\"\n",
    "    print(f\"\\nSplitting documents with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}...\")\n",
    "    \n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    doc_chunks_map = {}  # Maps doc_index to its chunks\n",
    "    \n",
    "    # Process each document separately to maintain boundaries\n",
    "    for doc in documents:\n",
    "        doc_index = doc.metadata[\"doc_index\"]\n",
    "        print(f\"Processing document {doc_index}: {doc.metadata['doc_name']}\")\n",
    "        \n",
    "        doc_chunks = text_splitter.split_documents([doc])\n",
    "        doc_chunks_map[doc_index] = doc_chunks\n",
    "        \n",
    "        # Add additional metadata to each chunk\n",
    "        for j, chunk in enumerate(doc_chunks):\n",
    "            # Copy existing metadata and add chunk-specific info\n",
    "            chunk.metadata[\"chunk_index\"] = j\n",
    "            chunk.metadata[\"total_chunks_in_doc\"] = len(doc_chunks)\n",
    "            chunk.metadata[\"is_first_chunk\"] = (j == 0)\n",
    "            chunk.metadata[\"is_last_chunk\"] = (j == len(doc_chunks) - 1)\n",
    "            \n",
    "            # Store adjacent document indices\n",
    "            chunk.metadata[\"prev_doc_index\"] = doc_index - 1 if doc_index > 0 else None\n",
    "            chunk.metadata[\"next_doc_index\"] = doc_index + 1 if doc_index < len(documents) - 1 else None\n",
    "            \n",
    "            all_chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} total chunks across {len(documents)} documents\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0c58f",
   "metadata": {},
   "source": [
    "## Retrive Adjacent Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8345c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacent_docs(chunks: List[Document], chunk_index: int, n: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get the current chunk and its adjacent chunks within the same document.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of all document chunks\n",
    "        chunk_index: Index of the current chunk\n",
    "        n: Number of adjacent chunks to retrieve in each direction (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the current chunk and lists of previous and next chunks\n",
    "    \"\"\"\n",
    "    if chunk_index < 0 or chunk_index >= len(chunks):\n",
    "        raise ValueError(f\"Chunk index {chunk_index} is out of bounds\")\n",
    "    \n",
    "    current_chunk = chunks[chunk_index]\n",
    "    current_doc_index = current_chunk.metadata[\"doc_index\"]\n",
    "    current_chunk_idx_in_doc = current_chunk.metadata[\"chunk_index\"]\n",
    "    \n",
    "    print(f\"\\nRetrieving adjacent chunks for chunk {chunk_index}:\")\n",
    "    print(f\"  Document: {current_doc_index} ({current_chunk.metadata['doc_name']})\")\n",
    "    print(f\"  Chunk position within document: {current_chunk_idx_in_doc + 1} of {current_chunk.metadata['total_chunks_in_doc']}\")\n",
    "    print(f\"  Retrieving {n} chunks before and after\")\n",
    "    \n",
    "    # Find all chunks from the current document\n",
    "    current_doc_chunks = [\n",
    "        chunk for chunk in chunks \n",
    "        if chunk.metadata[\"doc_index\"] == current_doc_index\n",
    "    ]\n",
    "    \n",
    "    # Sort chunks by their position in the document\n",
    "    current_doc_chunks.sort(key=lambda x: x.metadata[\"chunk_index\"])\n",
    "    \n",
    "    # Find the position of the current chunk within the document\n",
    "    current_position = -1\n",
    "    for i, chunk in enumerate(current_doc_chunks):\n",
    "        if chunk.metadata[\"chunk_index\"] == current_chunk.metadata[\"chunk_index\"]:\n",
    "            current_position = i\n",
    "            break\n",
    "    \n",
    "    if current_position == -1:\n",
    "        raise ValueError(\"Current chunk not found in its document chunks\")\n",
    "    \n",
    "    # Get previous n chunks (if available)\n",
    "    prev_chunks = []\n",
    "    start_idx = max(0, current_position - n)\n",
    "    if start_idx < current_position:\n",
    "        prev_chunks = current_doc_chunks[start_idx:current_position]\n",
    "    \n",
    "    # Get next n chunks (if available)\n",
    "    next_chunks = []\n",
    "    end_idx = min(len(current_doc_chunks), current_position + n + 1)\n",
    "    if current_position + 1 < end_idx:\n",
    "        next_chunks = current_doc_chunks[current_position + 1:end_idx]\n",
    "    \n",
    "    print(f\"  Found {len(prev_chunks)} previous chunks and {len(next_chunks)} next chunks\")\n",
    "    \n",
    "    return {\n",
    "        \"current_chunk\": current_chunk,\n",
    "        \"prev_chunks\": prev_chunks,\n",
    "        \"next_chunks\": next_chunks,\n",
    "        \"all_doc_chunks\": current_doc_chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d024bb",
   "metadata": {},
   "source": [
    "## Storing documents in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "320e535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def save_chunks_to_disk(chunks: List[Document], output_folder: str = \"/workspaces/RAG_BOT/LocalChunks\") -> str:\n",
    "    \"\"\"\n",
    "    Save document chunks to disk using pickle serialization.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks\n",
    "        output_folder: Folder to save the chunks\n",
    "        \n",
    "    Returns:\n",
    "        Path to the saved chunks file\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving {len(chunks)} chunks to disk...\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a filename with timestamp to avoid overwriting\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    chunks_path = os.path.join(output_folder, f\"document_chunks_{timestamp}.pkl\")\n",
    "    \n",
    "    # Save the chunks to disk\n",
    "    with open(chunks_path, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    \n",
    "    print(f\"Chunks saved to {chunks_path}\")\n",
    "    return chunks_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3f05e",
   "metadata": {},
   "source": [
    "## Loading local documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "865f0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def load_chunks_from_disk(chunks_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load document chunks from disk.\n",
    "    \n",
    "    Args:\n",
    "        chunks_path: Path to the saved chunks file\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading chunks from {chunks_path}...\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(chunks_path):\n",
    "        raise FileNotFoundError(f\"Chunks file not found at {chunks_path}\")\n",
    "    \n",
    "    # Load the chunks from disk\n",
    "    with open(chunks_path, \"rb\") as f:\n",
    "        chunks = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(chunks)} chunks from disk\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37811b06",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa50e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks: List[Document]) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and print information about the created chunks.\n",
    "    \"\"\"\n",
    "    print(\"\\nChunk Analysis:\")\n",
    "    \n",
    "    # Get unique document indices\n",
    "    doc_indices = sorted(set(chunk.metadata[\"doc_index\"] for chunk in chunks))\n",
    "    \n",
    "    # Count chunks per document\n",
    "    chunks_per_doc = {}\n",
    "    chunk_sizes = {}\n",
    "    \n",
    "    for doc_idx in doc_indices:\n",
    "        doc_chunks = [chunk for chunk in chunks if chunk.metadata[\"doc_index\"] == doc_idx]\n",
    "        chunks_per_doc[doc_idx] = len(doc_chunks)\n",
    "        \n",
    "        # Get document name from first chunk\n",
    "        doc_name = doc_chunks[0].metadata[\"doc_name\"] if doc_chunks else \"Unknown\"\n",
    "        \n",
    "        # Calculate chunk sizes\n",
    "        sizes = [len(chunk.page_content) for chunk in doc_chunks]\n",
    "        chunk_sizes[doc_idx] = {\n",
    "            \"doc_name\": doc_name,\n",
    "            \"min_size\": min(sizes) if sizes else 0,\n",
    "            \"max_size\": max(sizes) if sizes else 0,\n",
    "            \"avg_size\": sum(sizes) / len(sizes) if sizes else 0\n",
    "        }\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    analysis_data = []\n",
    "    for doc_idx in doc_indices:\n",
    "        analysis_data.append({\n",
    "            \"Document Index\": doc_idx,\n",
    "            \"Document Name\": chunk_sizes[doc_idx][\"doc_name\"],\n",
    "            \"Number of Chunks\": chunks_per_doc[doc_idx],\n",
    "            \"Min Chunk Size\": chunk_sizes[doc_idx][\"min_size\"],\n",
    "            \"Max Chunk Size\": chunk_sizes[doc_idx][\"max_size\"],\n",
    "            \"Avg Chunk Size\": chunk_sizes[doc_idx][\"avg_size\"]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(analysis_data)\n",
    "    print(df)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_chunks = len(chunks)\n",
    "    avg_chunks_per_doc = sum(chunks_per_doc.values()) / len(chunks_per_doc) if chunks_per_doc else 0\n",
    "    \n",
    "    print(f\"\\nTotal chunks: {total_chunks}\")\n",
    "    print(f\"Average chunks per document: {avg_chunks_per_doc:.2f}\")\n",
    "    print(f\"Total documents: {len(doc_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d9405",
   "metadata": {},
   "source": [
    "## Generate Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f064c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def generate_and_save_embeddings(chunks:List[Document], output_folder: str = \"/workspaces/RAG_BOT/LocalEmbeddings/Faiss_Embedding\") -> None:\n",
    "    \"\"\"\n",
    "    Generate embeddings for all chunks using HuggingFaceEmbeddings and save as FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks\n",
    "        output_folder: Folder to save the FAISS index\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "    \n",
    "    # Initialize the embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    print(f\"Using embedding model: all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Create FAISS index from documents\n",
    "    print(\"Creating FAISS index...\")\n",
    "    db = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Save the index to disk\n",
    "    index_path = os.path.join(output_folder, \"document_index\")\n",
    "    db.save_local(index_path)\n",
    "    print(f\"FAISS index saved to {index_path}\")\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4ed07",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11520f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading documents from /workspaces/RAG_BOT/DataEnriching/enriched_endpoints...\n",
      "Found 9 documents\n",
      "\n",
      "Document Details:\n",
      "Document 0: PolicyMangement_endpoint_001 - 3729 characters\n",
      "Document 1: PolicyMangement_endpoint_002 - 5203 characters\n",
      "Document 2: PolicyMangement_endpoint_003 - 5740 characters\n",
      "Document 3: PolicyMangement_endpoint_004 - 3595 characters\n",
      "Document 4: PolicyMangement_endpoint_005 - 5221 characters\n",
      "Document 5: PolicyMangement_endpoint_006 - 4011 characters\n",
      "Document 6: PolicyMangement_endpoint_007 - 3679 characters\n",
      "Document 7: PolicyMangement_endpoint_008 - 5121 characters\n",
      "Document 8: PolicyMangement_endpoint_009 - 4967 characters\n",
      "\n",
      "Splitting documents with chunk_size=1000, chunk_overlap=150...\n",
      "Processing document 1: PolicyMangement_endpoint_001\n",
      "Processing document 2: PolicyMangement_endpoint_002\n",
      "Processing document 3: PolicyMangement_endpoint_003\n",
      "Processing document 4: PolicyMangement_endpoint_004\n",
      "Processing document 5: PolicyMangement_endpoint_005\n",
      "Processing document 6: PolicyMangement_endpoint_006\n",
      "Processing document 7: PolicyMangement_endpoint_007\n",
      "Processing document 8: PolicyMangement_endpoint_008\n",
      "Processing document 9: PolicyMangement_endpoint_009\n",
      "Created 58 total chunks across 9 documents\n",
      "\n",
      "Saving 58 chunks to disk...\n",
      "Chunks saved to /workspaces/RAG_BOT/LocalChunks/document_chunks_20250715_020245.pkl\n",
      "\n",
      "Chunk Analysis:\n",
      "   Document Index                 Document Name  Number of Chunks  \\\n",
      "0               1  PolicyMangement_endpoint_001                 6   \n",
      "1               2  PolicyMangement_endpoint_002                 6   \n",
      "2               3  PolicyMangement_endpoint_003                 9   \n",
      "3               4  PolicyMangement_endpoint_004                 5   \n",
      "4               5  PolicyMangement_endpoint_005                 7   \n",
      "5               6  PolicyMangement_endpoint_006                 5   \n",
      "6               7  PolicyMangement_endpoint_007                 5   \n",
      "7               8  PolicyMangement_endpoint_008                 8   \n",
      "8               9  PolicyMangement_endpoint_009                 7   \n",
      "\n",
      "   Min Chunk Size  Max Chunk Size  Avg Chunk Size  \n",
      "0             180             983      646.833333  \n",
      "1             666             993      913.166667  \n",
      "2             279             995      663.222222  \n",
      "3             522             950      715.800000  \n",
      "4             555             988      775.428571  \n",
      "5             719             967      816.600000  \n",
      "6             582             952      734.200000  \n",
      "7             269             939      661.875000  \n",
      "8             229             965      724.857143  \n",
      "\n",
      "Total chunks: 58\n",
      "Average chunks per document: 6.44\n",
      "Total documents: 9\n",
      "\n",
      "Generating embeddings for 58 chunks...\n",
      "Using embedding model: all-MiniLM-L6-v2\n",
      "Creating FAISS index...\n",
      "FAISS index saved to /workspaces/RAG_BOT/LocalEmbeddings/Chatgpt_Enriched_Policy_Only_Embedding/document_index\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main(save_to_disk=True):\n",
    "    # Path to the enriched endpoints folder\n",
    "    folder_path = \"/workspaces/RAG_BOT/DataEnriching/enriched_endpoints\"\n",
    "    local_chunks_folder = \"/workspaces/RAG_BOT/LocalChunks\"\n",
    "    local_embedding_folder = \"/workspaces/RAG_BOT/LocalEmbeddings/Chatgpt_Enriched_Policy_Only_Embedding\"\n",
    "\n",
    "    # Read all documents\n",
    "    documents = read_documents(folder_path)\n",
    "    \n",
    "    # Print document names and sizes\n",
    "    print(\"\\nDocument Details:\")\n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"Document {i}: {doc.metadata['doc_name']} - {len(doc.page_content)} characters\")\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    chunks = split_documents(documents, chunk_size=1000, chunk_overlap=150)\n",
    "    \n",
    "    # Save chunks to disk if requested\n",
    "    if save_to_disk:\n",
    "        chunks_path = save_chunks_to_disk(chunks, local_chunks_folder)\n",
    "    \n",
    "    # Analyze chunk distribution\n",
    "    analyze_chunks(chunks)\n",
    "    \n",
    "    # Generate embeddings and save to FAISS index\n",
    "    db = generate_and_save_embeddings(chunks, output_folder=local_embedding_folder)\n",
    "    \n",
    "    return chunks, db\n",
    "\n",
    "chunks, db = main(save_to_disk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1328e6",
   "metadata": {},
   "source": [
    "## Load from local and Perform get adjacent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880e0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading chunks from /workspaces/RAG_BOT/LocalChunks/document_chunks_20250715_020245.pkl...\n",
      "Loaded 58 chunks from disk\n",
      "\n",
      "Retrieving adjacent chunks for chunk 3:\n",
      "  Document: 1 (PolicyMangement_endpoint_001)\n",
      "  Chunk position within document: 4 of 6\n",
      "  Retrieving 3 chunks before and after\n",
      "  Found 3 previous chunks and 2 next chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'current_chunk': Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 3, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='## Request Body  \\n```json\\n{\\n  \"uuid\": \"string_value\"\\n}\\n```\\n\\n## Response Parameters  \\n- **Result**  \\n  - **Type:** boolean  \\n  - **Description:** Indicates whether the deletion was successful.  \\n  - **Example Value:** `false`  \\n\\n- **Error**  \\n  - **Type:** object  \\n  - **Description:** Contains error message text if the deletion fails; may be null.  \\n  - **Example Value:** `{}`  \\n\\n## Sample Response JSON  \\n```json\\n{\\n  \"Result\": false,\\n  \"Error\": {}\\n}\\n```'),\n",
       " 'prev_chunks': [Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 0, 'total_chunks_in_doc': 6, 'is_first_chunk': True, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Overview  \\nThe **Delete Authentication Profile** endpoint allows users to remove an existing authentication profile from the system. This is done by sending a POST request to the path **/AuthProfile/DeleteProfile**. To access this endpoint, users must provide a valid bearer token for authentication.\\n\\n# Key Search Terms  \\n- Delete authentication profile  \\n- Remove auth profile  \\n- Authentication UUID deletion  \\n- API endpoint for deleting profiles  \\n- POST request to delete profile  \\n- Bearer token authentication  \\n- AuthProfile API  \\n- JSON request for profile deletion  \\n- Error handling in profile deletion  \\n- Authentication profile management'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 1, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Example User Questions  \\n- How can I delete an authentication profile?  \\n- What is the endpoint to remove an auth profile?  \\n- What parameters do I need to send to delete a profile?  \\n- How do I authenticate when deleting an authentication profile?  \\n- What does the response look like when I delete a profile?  \\n- Can I delete an authentication profile using a UUID?  \\n- What should I do if I receive an error when trying to delete a profile?  \\n- Is there a sample request for deleting an authentication profile?  \\n- What security measures are in place for this endpoint?  \\n- How do I know if the deletion was successful?'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 2, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Developer Notes  \\n- **Required Parameters:**  \\n  - `uuid` (query, Required): The UUID of the authentication profile to be deleted.  \\n- **Request Structure:**  \\n  - Content Type: `application/json`  \\n  - Request Body must include the `uuid`.  \\n- **Response Structure:**  \\n  - Status Code: `200`  \\n  - Response Body will contain:  \\n    - `Result` (boolean): Indicates success or failure of the delete operation.  \\n    - `Error` (object): Contains error message text on failure, may be null.  \\n- **Security Considerations:**  \\n  - This endpoint requires bearer token authentication.  \\n\\n# Detailed Explanation of Available Data, Request and Response Parameters  \\n## Request Parameters  \\n- **uuid**  \\n  - **Type:** string  \\n  - **Description:** The authentication profile UUID that identifies the profile to be deleted.  \\n  - **Constraints:** Required field.  \\n  - **Example Value:** `\"123e4567-e89b-12d3-a456-426614174000\"`  \\n\\n## Request Body  \\n```json\\n{\\n  \"uuid\": \"string_value\"\\n}\\n```')],\n",
       " 'next_chunks': [Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 4, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Raw Endpoint Documentation (Formatted)  \\n**ENDPOINT:** Delete authentication profile  \\n**PATH:** /AuthProfile/DeleteProfile  \\n**METHOD:** POST  \\n**TAGS:** Authentication Profile  \\n**METADATA:**  \\n  * x-idap-anon: False  \\n  * x-codegen-request-body-name: payload  \\n**PARAMETERS:**  \\n  * uuid (query, Required): Authentication UUID.  \\n**REQUEST BODY:** Required  \\n  Content Type: application/json  \\n  Schema Properties:  \\n    * uuid (string): The authentication profile uuid either passed in by method call or as part of the payload.  \\n    **Required fields:** uuid  \\n  **Sample Request JSON:**  \\n  ```json\\n  {\\n    \"uuid\": \"string_value\"\\n  }\\n  ```  \\n**RESPONSES:**  \\n  Status Code: 200  \\n  Description: API-Result  \\n  Content Type: */*  \\n  Response Schema: AuthProfileDeleteProfile  \\n  Response Body Properties:  \\n    * Result (boolean): Success or failure of the delete  \\n    * Error (object): Error message text on failure, may be null  \\n  **Sample Response JSON:**  \\n  ```json\\n  {'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 5, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': True, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='* Error (object): Error message text on failure, may be null  \\n  **Sample Response JSON:**  \\n  ```json\\n  {\\n    \"Result\": false,\\n    \"Error\": {}\\n  }\\n  ```  \\n**SECURITY:** bearerAuth')],\n",
       " 'all_doc_chunks': [Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 0, 'total_chunks_in_doc': 6, 'is_first_chunk': True, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Overview  \\nThe **Delete Authentication Profile** endpoint allows users to remove an existing authentication profile from the system. This is done by sending a POST request to the path **/AuthProfile/DeleteProfile**. To access this endpoint, users must provide a valid bearer token for authentication.\\n\\n# Key Search Terms  \\n- Delete authentication profile  \\n- Remove auth profile  \\n- Authentication UUID deletion  \\n- API endpoint for deleting profiles  \\n- POST request to delete profile  \\n- Bearer token authentication  \\n- AuthProfile API  \\n- JSON request for profile deletion  \\n- Error handling in profile deletion  \\n- Authentication profile management'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 1, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Example User Questions  \\n- How can I delete an authentication profile?  \\n- What is the endpoint to remove an auth profile?  \\n- What parameters do I need to send to delete a profile?  \\n- How do I authenticate when deleting an authentication profile?  \\n- What does the response look like when I delete a profile?  \\n- Can I delete an authentication profile using a UUID?  \\n- What should I do if I receive an error when trying to delete a profile?  \\n- Is there a sample request for deleting an authentication profile?  \\n- What security measures are in place for this endpoint?  \\n- How do I know if the deletion was successful?'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 2, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Developer Notes  \\n- **Required Parameters:**  \\n  - `uuid` (query, Required): The UUID of the authentication profile to be deleted.  \\n- **Request Structure:**  \\n  - Content Type: `application/json`  \\n  - Request Body must include the `uuid`.  \\n- **Response Structure:**  \\n  - Status Code: `200`  \\n  - Response Body will contain:  \\n    - `Result` (boolean): Indicates success or failure of the delete operation.  \\n    - `Error` (object): Contains error message text on failure, may be null.  \\n- **Security Considerations:**  \\n  - This endpoint requires bearer token authentication.  \\n\\n# Detailed Explanation of Available Data, Request and Response Parameters  \\n## Request Parameters  \\n- **uuid**  \\n  - **Type:** string  \\n  - **Description:** The authentication profile UUID that identifies the profile to be deleted.  \\n  - **Constraints:** Required field.  \\n  - **Example Value:** `\"123e4567-e89b-12d3-a456-426614174000\"`  \\n\\n## Request Body  \\n```json\\n{\\n  \"uuid\": \"string_value\"\\n}\\n```'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 3, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='## Request Body  \\n```json\\n{\\n  \"uuid\": \"string_value\"\\n}\\n```\\n\\n## Response Parameters  \\n- **Result**  \\n  - **Type:** boolean  \\n  - **Description:** Indicates whether the deletion was successful.  \\n  - **Example Value:** `false`  \\n\\n- **Error**  \\n  - **Type:** object  \\n  - **Description:** Contains error message text if the deletion fails; may be null.  \\n  - **Example Value:** `{}`  \\n\\n## Sample Response JSON  \\n```json\\n{\\n  \"Result\": false,\\n  \"Error\": {}\\n}\\n```'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 4, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': False, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='# Raw Endpoint Documentation (Formatted)  \\n**ENDPOINT:** Delete authentication profile  \\n**PATH:** /AuthProfile/DeleteProfile  \\n**METHOD:** POST  \\n**TAGS:** Authentication Profile  \\n**METADATA:**  \\n  * x-idap-anon: False  \\n  * x-codegen-request-body-name: payload  \\n**PARAMETERS:**  \\n  * uuid (query, Required): Authentication UUID.  \\n**REQUEST BODY:** Required  \\n  Content Type: application/json  \\n  Schema Properties:  \\n    * uuid (string): The authentication profile uuid either passed in by method call or as part of the payload.  \\n    **Required fields:** uuid  \\n  **Sample Request JSON:**  \\n  ```json\\n  {\\n    \"uuid\": \"string_value\"\\n  }\\n  ```  \\n**RESPONSES:**  \\n  Status Code: 200  \\n  Description: API-Result  \\n  Content Type: */*  \\n  Response Schema: AuthProfileDeleteProfile  \\n  Response Body Properties:  \\n    * Result (boolean): Success or failure of the delete  \\n    * Error (object): Error message text on failure, may be null  \\n  **Sample Response JSON:**  \\n  ```json\\n  {'),\n",
       "  Document(metadata={'source': '/workspaces/RAG_BOT/DataEnriching/enriched_endpoints/PolicyMangement_endpoint_001.txt', 'doc_name': 'PolicyMangement_endpoint_001', 'doc_index': 1, 'chunk_index': 5, 'total_chunks_in_doc': 6, 'is_first_chunk': False, 'is_last_chunk': True, 'prev_doc_index': 0, 'next_doc_index': 2}, page_content='* Error (object): Error message text on failure, may be null  \\n  **Sample Response JSON:**  \\n  ```json\\n  {\\n    \"Result\": false,\\n    \"Error\": {}\\n  }\\n  ```  \\n**SECURITY:** bearerAuth')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_chunks_path = \"/workspaces/RAG_BOT/LocalChunks/document_chunks_20250715_020245.pkl\"\n",
    "\n",
    "local_chunks = load_chunks_from_disk(local_chunks_path)\n",
    "\n",
    "get_adjacent_docs(chunks, 3, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221a205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
