{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ch2JmlLVgiX"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CJIaxT5AZYMK"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain-ollama langchain langchain-community faiss-cpu langchain_huggingface rank_bm25 gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzjvKsBqZcMY",
        "outputId": "37b28ba3-ffc4-41fd-ecc4-1750d4fac124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS vector store loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from re import search\n",
        "# Custom MultiQuery with Output Parser\n",
        "from typing import List\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "\n",
        "# Initialize the LLM\n",
        "from langchain_ollama import OllamaLLM\n",
        "llm = OllamaLLM(model=\"llama3.2:latest\", temperature=0.5)\n",
        "\n",
        "# Initialize the embedding model\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load the vector store\n",
        "from langchain.vectorstores import FAISS\n",
        "loaded_faiss_store = FAISS.load_local(\n",
        "    \"/content/RAG_BOT/LocalEmbeddings/huggingface_faiss_index\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "print(\"FAISS vector store loaded successfully.\")\n",
        "\n",
        "# Configure logging to see generated queries\n",
        "import logging\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "# Domain-specific prompt for CyberArk API documentation\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "You are helping retrieve answers from CyberArk API documentation.\n",
        "\n",
        "For general greetings or small talk (like \"hello\", \"hi\"), respond politely as a friendly assistant.\n",
        "\n",
        "For Cyberark API Documentation related question, Your task is to rewrite the user's question in 2 different ways, one should talk about the exact user intent and other shoulb be like a rephrased question of the orignial question without losing its intent and remember that the questins are about API's.\n",
        "\n",
        "Return each variant on its own line with no numbering or bullets.\n",
        "\n",
        "User question:\n",
        "{question}\n",
        "\n",
        "Variations:\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create chain and retriever\n",
        "llm_chain = QUERY_PROMPT | llm\n",
        "\n",
        "custom_multi_query_retriever = MultiQueryRetriever(\n",
        "    retriever=loaded_faiss_store.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\"score_threshold\" : 0.3 ,\"k\": 5}\n",
        "    ),\n",
        "    llm_chain=llm_chain,\n",
        "    parser_key=\"lines\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6oNTXXFaGT2E"
      },
      "outputs": [],
      "source": [
        "# Ensemble Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# Extract documents from the docstore\n",
        "try:\n",
        "    all_docs = [loaded_faiss_store.docstore._dict[doc_id] for doc_id in loaded_faiss_store.index_to_docstore_id.values()]\n",
        "except AttributeError:\n",
        "    # Fallback for different docstore structure\n",
        "    all_docs = [loaded_faiss_store.docstore.get(doc_id) for doc_id in loaded_faiss_store.index_to_docstore_id.values()]\n",
        "\n",
        "\n",
        "# Create BM25 retriever\n",
        "bm25_retriever = BM25Retriever.from_documents(all_docs)\n",
        "bm25_retriever.k = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eNkYroDpGgcl"
      },
      "outputs": [],
      "source": [
        "# Create ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, custom_multi_query_retriever],\n",
        "    weights=[0.4, 0.6]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9UvG7kjC4qVU"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
        "\n",
        "rate_limiter = InMemoryRateLimiter(\n",
        "    requests_per_second=0.1,\n",
        "    check_every_n_seconds= 0.1,\n",
        "    max_bucket_size = 10,\n",
        ")\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model = 'llama3.2:latest',\n",
        "    temperature=0,\n",
        "    rate_limiter= rate_limiter\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_E_YIH3k5LTY"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a highly knowledgeable CyberArk API documentation assistant. Your job is to answer developer questions accurately and clearly using only the provided API documentation context.\n",
        "\n",
        "For general greetings or small talk (like \"hello\", \"hi\"), respond politely as a friendly assistant.\n",
        "\n",
        "For Cyberark API documentation related questions, Your answers must follow these rules:\n",
        "\n",
        "1. Use only the given context. If the answer is not in the context, say \"I don't know based on the provided documentation.\"\n",
        "2. If the user asks about an endpoint, provide its details from the context including:\n",
        "   - Path and method\n",
        "   - Required parameters (query, path, body)\n",
        "   - Security requirements\n",
        "   - Request body schema (in JSON if available)\n",
        "   - Response body schema (in JSON if available)\n",
        "   - Sample request and response if present\n",
        "3. Be clear and structured:\n",
        "   - Use bullet points for properties\n",
        "   - Include code blocks for JSON\n",
        "4. Never invent or guess missing details.\n",
        "5. If the context includes multiple endpoints, select only the most relevant.\n",
        "6. For CyberArk API questions, use only the given context. If the answer is not in the context, say \"I don't know based on the provided documentation.\"\n",
        "\n",
        "Answer as if you are the official CyberArk API documentation.\n",
        "\"\"\"\n",
        "\n",
        "system_message = SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT)\n",
        "\n",
        "human_message = HumanMessagePromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are answering questions about CyberArk's API. Use the documentation context and the chat history.\n",
        "\n",
        "Documentation Context:\n",
        "----------------------\n",
        "{context}\n",
        "\n",
        "Chat History:\n",
        "----------------------\n",
        "{chat_history}\n",
        "\n",
        "New User Question:\n",
        "----------------------\n",
        "{question}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "CONVERSATIONAL_PROMPT = ChatPromptTemplate.from_messages([\n",
        "    system_message,\n",
        "    human_message\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qeFZRCd0G9YN"
      },
      "outputs": [],
      "source": [
        "def format_chat_history(history):\n",
        "    return \"\\n\".join([f\"{role}: {text}\" for role, text in history])\n",
        "\n",
        "# Add this helper early in your notebook\n",
        "GREETINGS = {\"hi\", \"hello\", \"hey\", \"greetings\", \"good morning\", \"good evening\", \"good afternoon\"}\n",
        "\n",
        "def is_greeting(text: str) -> bool:\n",
        "    return text.strip().lower() in GREETINGS\n",
        "\n",
        "async def get_answer(user_question, history, retriever, chat_model):\n",
        "    # Intercept greetings or very short generic inputs\n",
        "    if is_greeting(user_question):\n",
        "        return \"Hello! üëã How can I help you with CyberArk API documentation today?\"\n",
        "\n",
        "    if len(user_question.strip().split()) < 2:\n",
        "        return \"Could you please provide more details about your question related to the CyberArk API?\"\n",
        "\n",
        "    # Retrieval stage\n",
        "    try:\n",
        "        docs = retriever.get_relevant_documents(user_question)\n",
        "        if not docs:\n",
        "            return \"I couldn't find anything relevant in the documentation for that question. Can you rephrase or be more specific?\"\n",
        "\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error while retrieving documentation: {e}\"\n",
        "\n",
        "    # Format chat history\n",
        "    formatted_history = format_chat_history(history)\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = CONVERSATIONAL_PROMPT.format(\n",
        "        context=context,\n",
        "        chat_history=formatted_history,\n",
        "        question=user_question\n",
        "    )\n",
        "\n",
        "    # Call SLM\n",
        "    try:\n",
        "        answer = await chat_model.ainvoke(prompt)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error while generating answer: {e}\"\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJOcDTmLQqAR",
        "outputId": "7eaf0a98-f018-4290-c9e1-6c6026e88ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "qlIazp2bPT72",
        "outputId": "70133610-07ed-4163-837c-6a38031e57f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting CyberArk API ChatBot...\n",
            "üíª Check console for detailed logs when questions are asked\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://eab1c77a0a7aa60df9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eab1c77a0a7aa60df9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üîç USER QUERY: What is the request body for /Policy/SavePolicyBlock3?\n",
            "\n",
            "üîé RETRIEVING DOCUMENTS...\n",
            "‚ùå RETRIEVAL ERROR: [Errno 111] Connection refused\n",
            "\n",
            "‚è±Ô∏è TOTAL PROCESSING TIME: 0.00 seconds\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio\n",
        "import traceback\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def sync_get_answer(user_message, history):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"üîç USER QUERY: {user_message}\")\n",
        "\n",
        "    # Log chat history\n",
        "    formatted_history = [(\"User\", h[0]) if h[1] is None else (\"Assistant\", h[1]) for h in history]\n",
        "    if formatted_history:\n",
        "        print(\"\\nüìú CHAT HISTORY:\")\n",
        "        for role, text in formatted_history:\n",
        "            print(f\"  {role}: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Create a new event loop instead of getting the current one\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "\n",
        "        # Custom wrapper to capture retrieval info\n",
        "        async def get_answer_with_logging(user_question, history, retriever, llm_model):\n",
        "            # Handle greetings and short inputs\n",
        "            if is_greeting(user_question):\n",
        "                print(\"üëã Greeting detected, skipping retrieval\")\n",
        "                return \"Hello! üëã How can I help you with CyberArk API documentation today?\"\n",
        "\n",
        "            if len(user_question.strip().split()) < 2:\n",
        "                print(\"üìù Query too short, skipping retrieval\")\n",
        "                return \"Could you please provide more details about your question related to the CyberArk API?\"\n",
        "\n",
        "            # Retrieval stage\n",
        "            retrieval_start = time.time()\n",
        "            print(\"\\nüîé RETRIEVING DOCUMENTS...\")\n",
        "            try:\n",
        "                docs = retriever.get_relevant_documents(user_question)\n",
        "                retrieval_time = time.time() - retrieval_start\n",
        "                print(f\"‚è±Ô∏è Document retrieval time: {retrieval_time:.2f} seconds\")\n",
        "\n",
        "                if not docs:\n",
        "                    print(\"‚ùå No relevant documents found\")\n",
        "                    return \"I couldn't find anything relevant in the documentation for that question. Can you rephrase or be more specific?\"\n",
        "\n",
        "                print(f\"\\nüìÑ RETRIEVED {len(docs)} DOCUMENTS:\")\n",
        "                for i, doc in enumerate(docs[:3]):  # Show first 3 docs\n",
        "                    print(f\"Document {i+1} (excerpt): {doc.page_content[:200]}...\")\n",
        "                if len(docs) > 3:\n",
        "                    print(f\"...and {len(docs)-3} more documents\")\n",
        "\n",
        "                context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "                context_length = len(context)\n",
        "                print(f\"\\nüìä CONTEXT SIZE: {context_length} characters\")\n",
        "            except Exception as error:\n",
        "                print(f\"‚ùå RETRIEVAL ERROR: {error}\")\n",
        "                return f\"‚ùå Error while retrieving documentation: {error}\"\n",
        "\n",
        "            # Format chat history\n",
        "            formatted_history_text = format_chat_history(history)\n",
        "\n",
        "            # Build prompt\n",
        "            print(\"\\nüí¨ SENDING TO LLM...\")\n",
        "            llm_start = time.time()\n",
        "\n",
        "            # Call LLM\n",
        "            try:\n",
        "                answer = await llm_model.ainvoke(CONVERSATIONAL_PROMPT.format(\n",
        "                    context=context,\n",
        "                    chat_history=formatted_history_text,\n",
        "                    question=user_question\n",
        "                ))\n",
        "                llm_time = time.time() - llm_start\n",
        "                print(f\"‚è±Ô∏è LLM response time: {llm_time:.2f} seconds\")\n",
        "                print(f\"\\n‚úÖ ANSWER (first 100 chars): {str(answer)[:100]}...\")\n",
        "\n",
        "                # Extract content from AIMessage object if needed\n",
        "                if hasattr(answer, 'content'):\n",
        "                    return answer.content\n",
        "                return str(answer)  # Convert to string to ensure compatibility\n",
        "            except Exception as error:\n",
        "                print(f\"‚ùå LLM ERROR: {error}\")\n",
        "                return f\"‚ùå Error while generating answer: {error}\"\n",
        "\n",
        "        try:\n",
        "            answer = loop.run_until_complete(\n",
        "                get_answer_with_logging(user_message, formatted_history, ensemble_retriever, llm)\n",
        "            )\n",
        "        finally:\n",
        "            # Always close the loop, even if an exception occurs\n",
        "            loop.close()\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n‚è±Ô∏è TOTAL PROCESSING TIME: {total_time:.2f} seconds\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        return answer\n",
        "    except Exception as error:\n",
        "        tb = traceback.format_exc()\n",
        "        print(f\"‚ùå Error while generating answer: {error}\\n{tb}\")\n",
        "        return (\n",
        "            \"‚ö†Ô∏è Sorry, there was an error processing your request:\\n\"\n",
        "            f\"```\\n{tb}\\n```\"\n",
        "        )\n",
        "\n",
        "with gr.Blocks(title=\"üîê CyberArk API ChatBot\") as demo:\n",
        "    chatbot = gr.ChatInterface(\n",
        "        fn=sync_get_answer,\n",
        "        title=\"üîê CyberArk API ChatBot\",\n",
        "        description=(\n",
        "            \"Ask any question about CyberArk API documentation.\\n\"\n",
        "            \"Supports conversation context. For greetings, responds politely. \"\n",
        "            \"For API questions, retrieves details from documentation.\"\n",
        "        ),\n",
        "        examples=[\n",
        "            \"Hi\",\n",
        "            \"How do I delete an authentication profile?\",\n",
        "            \"What is the request body for /Policy/SavePolicyBlock3?\",\n",
        "            \"List all parameters for GetPolicyBlock\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "print(\"üöÄ Starting CyberArk API ChatBot...\")\n",
        "print(\"üíª Check console for detailed logs when questions are asked\")\n",
        "demo.launch(share=True,debug=True)  # Set share=True to get a public URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78NiAiM8KWWF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "async def debug_get_answer(user_question, history, retriever, llm):\n",
        "    \"\"\"\n",
        "    More robust helper to test get_answer() in Colab.\n",
        "    Prints context, timing, and errors if any.\n",
        "    \"\"\"\n",
        "    print(\"‚ö°Ô∏è Debugging get_answer() call\")\n",
        "    print(f\"User Question: {user_question}\")\n",
        "    print(f\"Chat History: {history}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        answer = await get_answer(user_question, history, retriever, llm)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        print(\"\\n‚úÖ Answer:\")\n",
        "        print(answer)\n",
        "        print(f\"\\n‚è±Ô∏è Response time: {duration:.2f} seconds\")\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(\"\\n‚ùå ERROR in get_answer():\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mHfU1F-K9CV",
        "outputId": "711a3f35-38e2-4873-f332-b53b81380875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö°Ô∏è Debugging get_answer() call\n",
            "User Question: How do I delete an authentication profile?\n",
            "Chat History: []\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "await debug_get_answer(\n",
        "    \"How do I delete an authentication profile?\",\n",
        "    history=[],\n",
        "    retriever=ensemble_retriever,\n",
        "    llm=llm\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}